---
layout: post
title: Week 11
---

## Validation: Artificial Benchmarks

A few weeks ago, we discussed community detection (i.e., clustering) and the existence of *many* algorithms for this purpose. Evaluating the quality of a clustering algorithm requires benchmark datasets with known ground truth community structures. Artificial benchmarks provide a controlled environment where the ground truth community structure is known, enabling the accurate assessment of the performance of community detection algorithms. This allows researchers to objectively compare different methods and measure their accuracy.

A famous benchmark, the Girvan and Newman (GN) benchmark (Girvan and Newman, 2002), has four evenly divide communities of 32 nodes each with an average degree of 16 edges. GN can't be considered a proxy for a real network, however, because each node has approximately the same degree and each community has the same size. Additionally, this network is too small to accurately gauge a clustering algorithm's performance on larger networks. 

An alternative benchmark, the Lancichinetti–Fortunato–Radicchi (LFR) benchmark, was published in 2008 that addressed these concerns (A. Lancichinetti, S. Fortunato, F. Radicchi, 2008). The LFR benchmark allows for the creation of networks with overlapping communities, variable community sizes, and diverse interconnectivity, making it more suitable for evaluating algorithms designed to handle real-world complexities.

## Reflections

My last (unofficial) assignment was helping a PhD student in our lab by providing her with a user experience report of her program that generates synthetic networks using the LFR software. This was the inspiration for this blog post! Aside from generating a few networks using her program, I wrote a quick script to calculate Normalized Mutual Information (NMI) to evaluate the clustering of the real-world network using the Scikit-learn Python library. 

The rest of my time this week was spent iteratively improving upon my proposal! I submitted another draft last week and will be incorporating the feedback I've received. The many iterations seem to have helped me refine my research question, but I'm still struggling with how I can effectively communicate this. 


